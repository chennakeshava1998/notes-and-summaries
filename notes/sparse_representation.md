## [Sparse Representation, AAAI 2019, Vincent, White](https://arxiv.org/pdf/1811.06626.pdf)
WIP
- [D] In *Background* section, you force the prediction to be a linear combination of the representations. This reduces the expressivity of the neural network. It also limits the posibilities of transfer-learning or fine-tuning later layers[Check!]. Is this assumption necessary? 

- [D] Sparse representation ensures stable value functions. But what about the rate of convergence to the true value?


### Doubts
1. What do you mean by catastrophic interference?
2. What do you mean by local representations?
3. How does sparsity ensure local representations?
4. WTH is tile coding? Check Sutton 1998
5. Fig.2 : Why is cumulative reward decreasing in Mountain Car for Vanilla Neural Network ??

